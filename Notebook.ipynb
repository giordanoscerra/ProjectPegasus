{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d91124f",
   "metadata": {},
   "source": [
    "# ProjectPegasus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8520f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Exam project for the Artificial Intelligence Fundamentals course, a.y. 2023/2024\n",
    "\n",
    "Notebook and code by the NetRiders team: Giordano Scerra, Andrea Marino, Yuri Ermes Negri, Davide Borghini, Davide Marchi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa7e9ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd81cd",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc86d5",
   "metadata": {},
   "source": [
    "This project focuses on the task of riding a pony. To do so, the agent:\n",
    "- Picks up some carrots\n",
    "- Feeds the pony with carrots, to pacify him and increase his tameness\n",
    "- Looks for a saddle\n",
    "- Applies the saddle on the pony and rides it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7dd98e",
   "metadata": {},
   "source": [
    "The one listed are not only steps that are necessary to perform the task successfully, but also *subtasks* on their own. As it will be thoroughly described later, the task is solved by selecting the most appropriate subtask and execute it. The execution of the subtask is controlled by a knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8972ebcf",
   "metadata": {},
   "source": [
    "## Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf0548",
   "metadata": {},
   "source": [
    "Our code is structured in three main files:\n",
    "- **Map.py**: These class' methods are used for the interactions with the environment: extracting information from the cells, rendering the level and making a proper step in the game (i.e. applying the actions).\n",
    "- **Agent.py**: Interacts with Map.py and the knowledge base with two methods: `percept()` and `act()`. \n",
    "    - `percept()`: reads information from the environment - using attributes from Map - and populates the knowledge base, using methods from the KBwrapper.\n",
    "    - `act()`: Queries the knowledge base for the best subtask to execute and launch the related methods.\n",
    "- **KBwrapper.py**: For the agent, its knowledge base is an instance of this class. Based on pyswip library, its methods are used to manage the \"kb.pl\" Prolog knowledge base by asserting, retracting and querying the appropriate predicates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f41f57",
   "metadata": {},
   "source": [
    "Our project is related with the course for these key topics:\n",
    "- A* search algorithm: we implemented A* to approach every element in the map and explore its every nook! We tried out different heuristics to assess their efficiency alongside it.\n",
    "- Knowledge Base: Our agent relies on a Knowledge Base to keep track of the environment that surrounds it, which is non-deterministic, and to remember basic but important things such as the category of the monsters it encounters, some rules of the game and directionality.\n",
    "- High Level Actions and Contingency planning in a non-deterministic environment: We decided to implement an agent that infers from its Knowledge Base subtasks, which may as well be HLAs, that complete particular, articulated tasks to reach a final state (the pony mounted) until a contingency is detected (through the agent's percepts) and the plan of the agent changes to reach its goal state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f819f",
   "metadata": {},
   "source": [
    "### Subtasks and Interrupts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8650c",
   "metadata": {},
   "source": [
    "Because of the non-deterministic nature of our environment, we decided to implement an intelligent system for decision making based on interruptions. As already mentioned, the `agent.act()` queries the knowledge base for the best subtask to perform among these ones:\n",
    "- *getCarrot* : to go and collect the nearest carrot\n",
    "- *getSaddle* : to go and collect the saddle, which is necessary to ride the pony \n",
    "- *feedSteed* : to feed the the pony by throwing carrots at it\n",
    "- *applySaddle* : to apply the saddle to the pony\n",
    "- *rideSteed* : to attempt to ride the pony\n",
    "- *explore* : to explore unseen zones of the map\n",
    "- *attackEnemy* : go towards an enemy and make a single attack at it\n",
    "- *eat* : take an item from the inventory and eat it to sate the agent's hunger or cure it from blindness\n",
    "\n",
    "Of course, each subtask requires to undertake a sequence of actions. After an action is performed, the knowledge base is queried again for an interruption signal, whose purpose is to notify the agent that the premises to continue that specific subtask are no longer valid and a subtask switch is needed. In particular, after every step the agent makes in the environment (basically after every `env.step`), the premises for the interruption of the current subtask are checked, and if satisfied the flow of actions required to perform the subtask is interrupted. After that, the knowledge base is queried again for the best subtask to perform next. \n",
    "\n",
    "We drew inspiration for the basic functioning of the agent in our project from the *knowledge_base_agent* paradygm that we saw during the course. In fact, whenever the agent has to do an action, it firsts senses the environment by calling the `percept()` method, which acts as a TELL, then performs an action, and finally queries the knowledge base for an interruption. Hence the ASK part is done in a two-fold way by both the interruption query and by the subtask query. \n",
    "\n",
    "We thought that this approach is both necessary for the task at hand, as the environment changes repeatedly and unpredictably due to things such as the activity of the pony and monsters, and interesting to see in action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4570c9a",
   "metadata": {},
   "source": [
    "### Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23fee1",
   "metadata": {},
   "source": [
    "As discussed during the oral presentation, and as described in the previous paragraph, the knowledge base is where both the information is stored and the decision making process for the agent takes place. This entity itself is divided in two parts, which can be distinguished by the two different languages in which they're written: Prolog and Python. In fact, Python is used as a wrapper interface to control the Prolog part, thanks to the open source library pyswip. \n",
    "The subtasks in the knowledge base are defined in order of relevance. We have:\n",
    "- getCarrot\n",
    "- feedSteed\n",
    "- getSaddle\n",
    "- applySaddle\n",
    "- explore (this subtask is chosen when no premise is satisfied for another subtask to start)\n",
    "- eat\n",
    "- attackMonster\n",
    "\n",
    "After the subtask's section, in our code we have the interruption's section. Here we can find, for every subtask, its own interruption clause.\n",
    "\n",
    "Then we have some facts, like the pony's maximum possible tameness assigned to 20, or the definition of a steed.\n",
    "\n",
    "Eventually, there is a section where we store some observations like the number of carrots or saddles in our possession, or the pony's current tameness. \n",
    "\n",
    "In the (rare) case where the agent has no better action than 'explore' and it performs 3 full map patrols, the subtask switches to riding. This was implemented to avoid endless exploring, it's a desperate last resort option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a32dfe",
   "metadata": {},
   "source": [
    "## Live demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9185d5e",
   "metadata": {},
   "source": [
    "Let's see a small demo of our project. In order to run the following code, and in general to replicate our work, make sure that:\n",
    "1. this notebook stays inside the folder in which it was delivered. This is necessary to resolve the imports\n",
    "2. the following packages that were used in the hands-on sessions are installed: `pyswip`, `minihack`, `numpy`, `nle`, `matplotlib`, `notebook` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f871b",
   "metadata": {},
   "source": [
    "Let's first import the classes that were described in the previous sections, and that are used in all the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.map import Map\n",
    "from utils.agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6906ec",
   "metadata": {},
   "source": [
    "We implemented three distances, to test them as heuristics for the A* algoritm. We ran all of our tests using `manhattan_distance`, but other heuristics can be tried out by passing them as parameters to `agent.act()`\n",
    "\n",
    "Of course, the correctness of the algorithm is independent from the used heuristic, as long as it is a correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.heuristics import manhattan_distance, infinity_distance, euclidean_distance\n",
    "heuristic = manhattan_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9172c",
   "metadata": {},
   "source": [
    "We provided plenty of examples to explore, and a lot of parameters to set at will. Let's create a simple map in which there are three enemies (a lichen, a jackal and a newt), an untamed pony and a saddle.\n",
    "\n",
    "Let's also create the agent, and first sense the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = Map(level=3, pony=True, enemies=True)\n",
    "\n",
    "steinbeck = Agent()\n",
    "steinbeck.percept(game_map=level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0206429",
   "metadata": {},
   "source": [
    "Let's display the map..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the graphic=True can be used only if level in [0,1,2,3] in the Map class\n",
    "level.render(graphic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f32185",
   "metadata": {},
   "source": [
    "... and run the example. This is as simple as asking the agent to act in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(not level.is_episode_over()):\n",
    "    steinbeck.act(level, show_steps=True, graphic=True, delay=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b90bdb",
   "metadata": {},
   "source": [
    "Let's see how well our valiant Steinbeck performed in this level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(level.rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a7369",
   "metadata": {},
   "source": [
    "Here, we sum all the rewards coming from the events we created.\n",
    "\n",
    "First of all, why a sum? \n",
    "Since we decided to append the reward assigned in each state of the environment to a list, we end up with a pretty long one, especially in gargantuan mazes.\n",
    "The events we defined and chose to record are `getCarrots` and `MountEvent` which respectively assign a reward of one point for picking up a carrot and 1000 points for mounting the steed.\n",
    "\n",
    "We discussed extensively about using an event that'd give off a reward of -1 for each step taken by the agent and came to the conclusion that it would be inadequate for our task: we wished to face multiple contingencies while taming the steed: fighting a monster in our path to a carrot or near the steed, finding all the carrots, getting the steed to drop the saddle if it ever wished to take it and many more. Because of this, using such a reward would have pushed us to solve the task *faster* instead of *better*, becoming a sort of toxic inductive bias on our work: we'd have been more focused on finding good heuristics to finish the level in time instead of solving the many problems we had to face. \n",
    "Also, since the maps can be different both in size and complexity, adding such a metric would meddle the evaluations of the performance of our agent: on bigger, more difficult maps to cross and explore, the agent would forcibly be penalized even if it managed to find all the carrots (which maximize its probability to apply the saddle to the pony and mount it), kill any monster that put it or its steed in danger or even cure itself from blindness or the hunger states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470c341",
   "metadata": {},
   "source": [
    "## Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad45b3c",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c1a43",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4488ec8",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194170e6",
   "metadata": {},
   "source": [
    "The github repository can be found [here](https://github.com/giordanoscerra/ProjectPegasus)\n",
    "\n",
    "Aside the statistics that can be easily seen from the repository, our team had real-life meetings to discuss which task to face and had many sessions over Discord in which we jointly wrote code and discussed on how to develop, fix and tackle the different problems that arose during our work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec56b87",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
